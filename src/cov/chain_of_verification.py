ab"""
Implementa√ß√£o do Chain of Verification (CoV) para aumentar assertividade das respostas da IA
"""

import json
from typing import Dict, Any, Tuple, Optional
from openai import OpenAI
from src.core.prompt_config import PromptConfig


class ChainOfVerification:
    """
    Implementa o padr√£o Chain of Verification para auto-cr√≠tica e corre√ß√£o de respostas da IA
    
    Fluxo:
    1. Resposta Inicial: AI gera resposta baseada no input
    2. Verifica√ß√£o: AI analisa sua pr√≥pria resposta buscando erros/melhorias
    3. Resposta Final: AI corrige ou confirma baseado na verifica√ß√£o
    """
    
    def __init__(self, client: OpenAI, prompts: PromptConfig):
        """
        Inicializa o sistema de Chain of Verification
        
        Args:
            client: Cliente OpenAI configurado
            prompts: Configura√ß√£o de prompts
        """
        self.client = client
        self.prompts = prompts
        self.verification_prompts = self._load_verification_prompts()
    
    def _load_verification_prompts(self) -> Dict[str, str]:
        """Carrega prompts espec√≠ficos para verifica√ß√£o"""
        return {
            "general_verification": """
Analise a resposta anterior e identifique poss√≠veis problemas:

1. PRECIS√ÉO: A informa√ß√£o est√° correta?
2. COMPLETUDE: Falta alguma informa√ß√£o importante?
3. CLAREZA: A resposta √© clara e bem estruturada?
4. CONTEXTO: A resposta atende ao que foi perguntado?
5. FUN√á√ïES: Se uma fun√ß√£o foi chamada, os par√¢metros est√£o corretos?

Responda em JSON:
{
    "has_issues": boolean,
    "issues": ["lista de problemas encontrados"],
    "suggestions": ["lista de melhorias sugeridas"],
    "severity": "low|medium|high",
    "should_regenerate": boolean
}
""",
            
            "function_verification": """
Analise especificamente a chamada de fun√ß√£o na resposta anterior:

1. PAR√ÇMETROS: Todos os par√¢metros necess√°rios est√£o presentes?
2. VALORES: Os valores dos par√¢metros s√£o apropriados?
3. CONTEXTO: A fun√ß√£o escolhida √© adequada para a pergunta?
4. VALIDA√á√ÉO: A resposta atende aos requisitos da fun√ß√£o?

Responda em JSON:
{
    "function_correct": boolean,
    "missing_params": ["lista de par√¢metros faltantes"],
    "invalid_params": ["lista de par√¢metros inv√°lidos"],
    "alternative_function": "nome_da_funcao_alternativa_se_aplicavel",
    "should_retry": boolean
}
""",
            
            "response_verification": """
Analise a qualidade da resposta final:

1. HUMOR: A resposta mant√©m o tom humor√≠stico apropriado para The Office?
2. INFORMA√á√ÉO: As informa√ß√µes est√£o completas e √∫teis?
3. FORMATO: A resposta est√° bem formatada?
4. ENGAJAMENTO: A resposta √© envolvente para o usu√°rio?

Responda em JSON:
{
    "quality_score": "1-10",
    "strengths": ["pontos fortes da resposta"],
    "weaknesses": ["pontos fracos da resposta"],
    "improvements": ["melhorias espec√≠ficas sugeridas"],
    "regenerate_recommended": boolean
}
"""
        }
    
    def verify_initial_response(self, user_input: str, initial_response: str, 
                              function_call: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Executa verifica√ß√£o da resposta inicial
        
        Args:
            user_input: Input original do usu√°rio
            initial_response: Resposta inicial da AI
            function_call: Informa√ß√µes sobre chamada de fun√ß√£o (se houver)
            
        Returns:
            Resultado da verifica√ß√£o
        """
        print("üîç [CoV] Iniciando verifica√ß√£o da resposta inicial...")
        
        # Monta contexto para verifica√ß√£o
        verification_context = f"""
INPUT DO USU√ÅRIO: {user_input}

RESPOSTA INICIAL: {initial_response}
"""
        
        if function_call:
            verification_context += f"""
FUN√á√ÉO CHAMADA: {function_call.get('name', 'N/A')}
PAR√ÇMETROS: {json.dumps(function_call.get('arguments', {}), indent=2)}
"""
        
        # Escolhe prompt de verifica√ß√£o apropriado
        if function_call:
            verification_prompt = self.verification_prompts["function_verification"]
        else:
            verification_prompt = self.verification_prompts["general_verification"]
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": "Voc√™ √© um cr√≠tico especializado em analisar respostas de IA. "
                                 "Seja rigoroso mas construtivo na sua an√°lise."
                    },
                    {
                        "role": "user", 
                        "content": f"{verification_context}\n\n{verification_prompt}"
                    }
                ],
                temperature=0.3  # Baixa temperatura para an√°lise mais consistente
            )
            
            verification_text = response.choices[0].message.content
            
            # Tenta parsear JSON
            try:
                verification_result = json.loads(verification_text)
                print(f"‚úÖ [CoV] Verifica√ß√£o conclu√≠da - Issues: {verification_result.get('has_issues', False)}")
                return verification_result
            except json.JSONDecodeError:
                print("‚ö†Ô∏è [CoV] Erro ao parsear JSON da verifica√ß√£o, usando an√°lise textual")
                return {
                    "has_issues": True,
                    "issues": ["Formato de resposta inv√°lido"],
                    "verification_text": verification_text,
                    "should_regenerate": False
                }
                
        except Exception as e:
            print(f"‚ùå [CoV] Erro na verifica√ß√£o: {str(e)}")
            return {
                "has_issues": False,
                "error": str(e),
                "should_regenerate": False
            }
    
    def generate_corrected_response(self, user_input: str, initial_response: str, 
                                  verification_result: Dict[str, Any],
                                  function_call: Optional[Dict[str, Any]] = None) -> str:
        """
        Gera resposta corrigida baseada na verifica√ß√£o
        
        Args:
            user_input: Input original do usu√°rio
            initial_response: Resposta inicial
            verification_result: Resultado da verifica√ß√£o
            function_call: Informa√ß√µes sobre chamada de fun√ß√£o
            
        Returns:
            Resposta corrigida
        """
        print("üîß [CoV] Gerando resposta corrigida...")
        
        # Monta prompt de corre√ß√£o
        correction_context = f"""
INPUT ORIGINAL: {user_input}

RESPOSTA INICIAL: {initial_response}

PROBLEMAS IDENTIFICADOS:
{json.dumps(verification_result, indent=2, ensure_ascii=False)}

Baseado nos problemas identificados, gere uma resposta melhorada que:
1. Corrija os erros apontados
2. Mantenha o tom humor√≠stico do DunderOps Assistant
3. Seja mais precisa e completa
4. Atenda melhor ao que o usu√°rio perguntou

Mantenha o estilo The Office e seja √∫til!
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": self.prompts.system_prompt
                    },
                    {
                        "role": "user",
                        "content": correction_context
                    }
                ],
                temperature=0.7  # Um pouco mais de criatividade para corre√ß√£o
            )
            
            corrected_response = response.choices[0].message.content
            print("‚úÖ [CoV] Resposta corrigida gerada")
            return corrected_response
            
        except Exception as e:
            print(f"‚ùå [CoV] Erro ao gerar corre√ß√£o: {str(e)}")
            # Fallback para resposta original se houver erro
            return initial_response
    
    def process_with_verification(self, user_input: str, initial_response: str,
                                function_call: Optional[Dict[str, Any]] = None) -> Tuple[str, Dict[str, Any]]:
        """
        Processo completo de Chain of Verification
        
        Args:
            user_input: Input do usu√°rio
            initial_response: Resposta inicial da AI
            function_call: Informa√ß√µes sobre chamada de fun√ß√£o
            
        Returns:
            Tuple[str, Dict]: (resposta_final, metadados_verificacao)
        """
        print("üîç [CoV] Iniciando Chain of Verification...")
        
        # Etapa 1: Verifica√ß√£o
        verification_result = self.verify_initial_response(
            user_input, initial_response, function_call
        )
        
        # Metadados da verifica√ß√£o
        verification_metadata = {
            "verification_performed": True,
            "verification_result": verification_result,
            "correction_applied": False,
            "verification_tokens_used": 0  # Ser√° atualizado pelo tracker
        }
        
        # Etapa 2: Decis√£o de corre√ß√£o inteligente
        should_correct = False
        
        # Adjust logic for trivia questions
        if verification_result.get("has_issues", False):
            severity = verification_result.get("severity", "medium")
            should_regenerate = verification_result.get("should_regenerate", False)
            is_trivia = verification_result.get("context", {}).get("type") == "trivia"

            # System threshold for trivia questions
            correction_threshold = self._get_correction_threshold(function_call)

            # Elevate severity for trivia with unmet criteria
            if is_trivia:
                unmet_criteria = verification_result.get("unmet_criteria", [])
                if unmet_criteria:
                    severity = "high" if "critical" not in unmet_criteria else "critical"

            # Apply stricter correction logic for trivia
            if is_trivia and severity in ["high", "critical"]:
                should_correct = True
                print(f"üîß [CoV] Aplicando corre√ß√£o em trivia - Severidade: {severity}")
            elif should_regenerate and severity == "high":
                should_correct = True
                print(f"üîß [CoV] Aplicando corre√ß√£o (regenerate + high) - Severidade: {severity}")
            elif severity == "critical":
                should_correct = True
                print(f"üîß [CoV] Aplicando corre√ß√£o (critical) - Severidade: {severity}")
            elif severity == "high" and correction_threshold == "strict":
                should_correct = True
                print(f"üîß [CoV] Aplicando corre√ß√£o (high + strict) - Severidade: {severity}")
            else:
                print(f"üìã [CoV] Verifica√ß√£o detectou issues (severity: {severity}) mas n√£o atende threshold para corre√ß√£o")
        
        # Etapa 3: Corre√ß√£o (se necess√°ria)
        if should_correct:
            corrected_response = self.generate_corrected_response(
                user_input, initial_response, verification_result, function_call
            )
            verification_metadata["correction_applied"] = True
            final_response = corrected_response
        else:
            print("‚úÖ [CoV] Resposta inicial aprovada na verifica√ß√£o")
            final_response = initial_response
        
        print(f"üèÅ [CoV] Chain of Verification conclu√≠do - Corre√ß√£o aplicada: {verification_metadata['correction_applied']}")
        
        return final_response, verification_metadata
    
    def _get_correction_threshold(self, function_call: Optional[Dict[str, Any]]) -> str:
        """
        Determina o threshold de corre√ß√£o baseado no contexto
        
        Returns:
            str: "strict", "moderate", ou "lenient"
        """
        if not function_call:
            # Respostas diretas: threshold leniente (raramente corrige)
            return "lenient"
        
        function_name = function_call.get("name", "")
        
        # Configura√ß√£o por fun√ß√£o
        function_thresholds = {
            "schedule_meeting": "strict",        # Meetings precisam estar corretos
            "generate_paper_quote": "strict",    # C√°lculos precisam estar corretos  
            "prank_dwight": "moderate"           # Humor pode ser mais flex√≠vel
        }
        
        return function_thresholds.get(function_name, "moderate")


class CoVConfiguration:
    """Configura√ß√µes espec√≠ficas para Chain of Verification"""
    
    def __init__(self):
        self.verification_enabled = True
        self.verification_threshold = "high"  # Mudan√ßa: de medium para high
        self.max_verification_attempts = 1
        self.verification_temperature = 0.3
        self.correction_temperature = 0.7
        
        # Configura√ß√µes por tipo de fun√ß√£o com thresholds mais rigorosos
        self.function_specific_config = {
            "schedule_meeting": {
                "verification_focus": ["completeness", "date_format", "time_validity"],
                "correction_priority": "high",
                "correction_threshold": "strict"
            },
            "generate_paper_quote": {
                "verification_focus": ["calculation_accuracy", "parameter_completeness"],
                "correction_priority": "high", 
                "correction_threshold": "strict"
            },
            "prank_dwight": {
                "verification_focus": ["humor_appropriateness", "creativity"],
                "correction_priority": "medium",
                "correction_threshold": "moderate"
            }
        }
        
        # Configura√ß√µes para respostas diretas (sem function calling)
        self.direct_response_config = {
            "correction_priority": "high",
            "correction_threshold": "strict",
            "verification_focus": ["accuracy", "helpfulness", "completeness", "context"]
        }
    
    def get_function_config(self, function_name: str) -> Dict[str, Any]:
        """Retorna configura√ß√£o espec√≠fica para uma fun√ß√£o"""
        return self.function_specific_config.get(function_name, {
            "verification_focus": ["general"],
            "correction_priority": "medium",
            "correction_threshold": "moderate"
        })
    
    def get_direct_response_config(self) -> Dict[str, Any]:
        """Retorna configura√ß√£o para respostas diretas (sem function calling)"""
        return self.direct_response_config
    
    def should_verify(self, function_name: Optional[str] = None) -> bool:
        """Determina se deve aplicar verifica√ß√£o baseado em configura√ß√µes mais inteligentes"""
        if not self.verification_enabled:
            return False
        
        if function_name:
            config = self.get_function_config(function_name)
            return config.get("correction_priority") != "low"
        else:
            # Para respostas diretas, usa configura√ß√£o espec√≠fica
            direct_config = self.get_direct_response_config()
            return direct_config.get("correction_priority") != "low"
