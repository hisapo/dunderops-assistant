"""
Sistema de logging para experimentos de comparaÃ§Ã£o entre implementaÃ§Ãµes
"""

import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

from .metrics_tracker import MetricData, MetricsAnalyzer


class ExperimentLogger:
    """Classe para salvar e carregar resultados de experimentos"""
    
    def __init__(self, experiments_dir: str = "experiments"):
        """
        Inicializa o logger de experimentos
        
        Args:
            experiments_dir: DiretÃ³rio onde salvar os experimentos
        """
        self.experiments_dir = Path(experiments_dir)
        self.experiments_dir.mkdir(exist_ok=True)
        
        # Cria subdiretÃ³rios se nÃ£o existirem
        (self.experiments_dir / "raw_data").mkdir(exist_ok=True)
        (self.experiments_dir / "comparisons").mkdir(exist_ok=True)
        (self.experiments_dir / "reports").mkdir(exist_ok=True)
        
        print(f"ğŸ“ [ExperimentLogger] DiretÃ³rio configurado: {self.experiments_dir}")
    
    def save_metrics(self, metrics: List[MetricData], 
                    implementation_type: str, 
                    experiment_name: Optional[str] = None) -> str:
        """
        Salva mÃ©tricas brutas de uma implementaÃ§Ã£o
        
        Args:
            metrics: Lista de mÃ©tricas coletadas
            implementation_type: Tipo da implementaÃ§Ã£o ("original" ou "cov")
            experiment_name: Nome do experimento (opcional)
            
        Returns:
            Caminho do arquivo salvo
        """
        if not experiment_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"experiment_{timestamp}"
        
        filename = f"{experiment_name}_{implementation_type}_metrics.json"
        filepath = self.experiments_dir / "raw_data" / filename
        
        # Converte mÃ©tricas para dicionÃ¡rio serializÃ¡vel
        data = {
            "experiment_name": experiment_name,
            "implementation_type": implementation_type,
            "timestamp": datetime.now().isoformat(),
            "total_executions": len(metrics),
            "metrics": [metric.to_dict() for metric in metrics]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ [ExperimentLogger] MÃ©tricas salvas: {filepath}")
        return str(filepath)
    
    def load_metrics(self, filepath: str) -> List[MetricData]:
        """
        Carrega mÃ©tricas de um arquivo
        
        Args:
            filepath: Caminho do arquivo
            
        Returns:
            Lista de mÃ©tricas carregadas
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        metrics = []
        for metric_dict in data["metrics"]:
            # ReconstrÃ³i o objeto MetricData
            metric = MetricData(**metric_dict)
            metrics.append(metric)
        
        print(f"ğŸ“‚ [ExperimentLogger] MÃ©tricas carregadas: {len(metrics)} execuÃ§Ãµes")
        return metrics
    
    def save_comparison(self, original_metrics: List[MetricData], 
                       cov_metrics: List[MetricData],
                       experiment_name: Optional[str] = None) -> str:
        """
        Salva uma comparaÃ§Ã£o completa entre implementaÃ§Ãµes
        
        Args:
            original_metrics: MÃ©tricas da implementaÃ§Ã£o original
            cov_metrics: MÃ©tricas da implementaÃ§Ã£o CoV
            experiment_name: Nome do experimento
            
        Returns:
            Caminho do arquivo de comparaÃ§Ã£o salvo
        """
        if not experiment_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"comparison_{timestamp}"
        
        # Gera comparaÃ§Ã£o usando o analyzer
        comparison = MetricsAnalyzer.compare_implementations(original_metrics, cov_metrics)
        
        # Adiciona dados brutos
        comparison_data = {
            "experiment_name": experiment_name,
            "timestamp": datetime.now().isoformat(),
            "comparison": comparison,
            "raw_data": {
                "original_metrics": [m.to_dict() for m in original_metrics],
                "cov_metrics": [m.to_dict() for m in cov_metrics]
            }
        }
        
        # Salva comparaÃ§Ã£o
        comparison_file = self.experiments_dir / "comparisons" / f"{experiment_name}.json"
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)
        
        # Gera e salva relatÃ³rio legÃ­vel
        report = MetricsAnalyzer.generate_report(comparison)
        report_file = self.experiments_dir / "reports" / f"{experiment_name}_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“Š [ExperimentLogger] ComparaÃ§Ã£o salva: {comparison_file}")
        print(f"ğŸ“„ [ExperimentLogger] RelatÃ³rio salvo: {report_file}")
        
        return str(comparison_file)
    
    def load_comparison(self, filepath: str) -> Dict[str, Any]:
        """
        Carrega uma comparaÃ§Ã£o salva
        
        Args:
            filepath: Caminho do arquivo de comparaÃ§Ã£o
            
        Returns:
            Dados da comparaÃ§Ã£o
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def list_experiments(self) -> Dict[str, List[str]]:
        """
        Lista todos os experimentos salvos
        
        Returns:
            DicionÃ¡rio com listas de arquivos por categoria
        """
        result = {
            "raw_metrics": [],
            "comparisons": [],
            "reports": []
        }
        
        # Lista mÃ©tricas brutas
        raw_dir = self.experiments_dir / "raw_data"
        if raw_dir.exists():
            result["raw_metrics"] = [f.name for f in raw_dir.glob("*.json")]
        
        # Lista comparaÃ§Ãµes
        comp_dir = self.experiments_dir / "comparisons"
        if comp_dir.exists():
            result["comparisons"] = [f.name for f in comp_dir.glob("*.json")]
        
        # Lista relatÃ³rios
        report_dir = self.experiments_dir / "reports"
        if report_dir.exists():
            result["reports"] = [f.name for f in report_dir.glob("*.txt")]
        
        return result
    
    def get_experiment_summary(self) -> str:
        """
        Gera um resumo de todos os experimentos
        
        Returns:
            String com resumo dos experimentos
        """
        experiments = self.list_experiments()
        
        summary = []
        summary.append("ğŸ“š RESUMO DOS EXPERIMENTOS")
        summary.append("=" * 40)
        summary.append(f"ğŸ“Š MÃ©tricas brutas: {len(experiments['raw_metrics'])}")
        summary.append(f"âš–ï¸ ComparaÃ§Ãµes: {len(experiments['comparisons'])}")
        summary.append(f"ğŸ“„ RelatÃ³rios: {len(experiments['reports'])}")
        summary.append("")
        
        if experiments["comparisons"]:
            summary.append("ğŸ” COMPARAÃ‡Ã•ES DISPONÃVEIS:")
            for comp in sorted(experiments["comparisons"]):
                summary.append(f"   â€¢ {comp}")
        
        return "\n".join(summary)
    
    def cleanup_old_experiments(self, days_old: int = 30) -> int:
        """
        Remove experimentos antigos
        
        Args:
            days_old: Idade em dias para considerar "antigo"
            
        Returns:
            NÃºmero de arquivos removidos
        """
        import time
        
        cutoff_time = time.time() - (days_old * 24 * 60 * 60)
        removed_count = 0
        
        for directory in [self.experiments_dir / "raw_data", 
                         self.experiments_dir / "comparisons",
                         self.experiments_dir / "reports"]:
            if directory.exists():
                for file_path in directory.glob("*"):
                    if file_path.stat().st_mtime < cutoff_time:
                        file_path.unlink()
                        removed_count += 1
        
        print(f"ğŸ§¹ [ExperimentLogger] Removidos {removed_count} arquivos antigos")
        return removed_count


class ExperimentSession:
    """Classe para gerenciar uma sessÃ£o de experimento completa"""
    
    def __init__(self, experiment_name: str, logger: ExperimentLogger):
        """
        Inicializa uma sessÃ£o de experimento
        
        Args:
            experiment_name: Nome do experimento
            logger: Instance do ExperimentLogger
        """
        self.experiment_name = experiment_name
        self.logger = logger
        self.original_metrics: List[MetricData] = []
        self.cov_metrics: List[MetricData] = []
        self.start_time = datetime.now()
        
        print(f"ğŸ¯ [ExperimentSession] Iniciando experimento: {experiment_name}")
    
    def add_original_metric(self, metric: MetricData):
        """Adiciona uma mÃ©trica da implementaÃ§Ã£o original"""
        self.original_metrics.append(metric)
        print(f"ğŸ“ˆ [ExperimentSession] MÃ©trica original adicionada "
              f"({len(self.original_metrics)} total)")
    
    def add_cov_metric(self, metric: MetricData):
        """Adiciona uma mÃ©trica da implementaÃ§Ã£o CoV"""
        self.cov_metrics.append(metric)
        print(f"ğŸ” [ExperimentSession] MÃ©trica CoV adicionada "
              f"({len(self.cov_metrics)} total)")
    
    def finalize_experiment(self) -> str:
        """
        Finaliza o experimento e salva todos os dados
        
        Returns:
            Caminho do arquivo de comparaÃ§Ã£o gerado
        """
        duration = datetime.now() - self.start_time
        
        print(f"ğŸ [ExperimentSession] Finalizando experimento {self.experiment_name}")
        print(f"   â€¢ DuraÃ§Ã£o: {duration}")
        print(f"   â€¢ MÃ©tricas originais: {len(self.original_metrics)}")
        print(f"   â€¢ MÃ©tricas CoV: {len(self.cov_metrics)}")
        
        # Salva mÃ©tricas brutas
        if self.original_metrics:
            self.logger.save_metrics(
                self.original_metrics, "original", self.experiment_name
            )
        
        if self.cov_metrics:
            self.logger.save_metrics(
                self.cov_metrics, "cov", self.experiment_name
            )
        
        # Salva comparaÃ§Ã£o se ambas existem
        if self.original_metrics and self.cov_metrics:
            comparison_file = self.logger.save_comparison(
                self.original_metrics, self.cov_metrics, self.experiment_name
            )
            return comparison_file
        
        print("âš ï¸ [ExperimentSession] NÃ£o foi possÃ­vel gerar comparaÃ§Ã£o - "
              "dados insuficientes")
        return ""
    
    def get_current_status(self) -> str:
        """Retorna status atual do experimento"""
        duration = datetime.now() - self.start_time
        
        return (f"ğŸ¯ Experimento: {self.experiment_name}\n"
                f"â±ï¸ DuraÃ§Ã£o: {duration}\n"
                f"ğŸ“Š Original: {len(self.original_metrics)} execuÃ§Ãµes\n"
                f"ğŸ” CoV: {len(self.cov_metrics)} execuÃ§Ãµes")
