"""
Sistema de rastreamento de m√©tricas para compara√ß√£o de implementa√ß√µes
"""

import time
import uuid
from dataclasses import dataclass, asdict
from typing import Dict, Any, Optional, List
from datetime import datetime


@dataclass
class MetricData:
    """Estrutura para armazenar m√©tricas de uma execu√ß√£o"""
    
    # Identifica√ß√£o
    execution_id: str
    timestamp: str
    implementation_type: str  # "original" ou "chain_of_verification"
    
    # Input/Output
    user_input: str
    final_response: str = ""
    function_called: Optional[str] = None
    function_params: Optional[Dict[str, Any]] = None
    function_result: Optional[Any] = None
    
    # M√©tricas de Performance
    total_latency_ms: float = 0.0
    api_calls_count: int = 0
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_tokens: int = 0
    
    # M√©tricas de Qualidade
    validation_passed: bool = False
    has_function_call: bool = False
    response_complete: bool = True
    
    # M√©tricas espec√≠ficas do Chain of Verification
    verification_used: bool = False
    verification_latency_ms: float = 0.0
    verification_tokens: int = 0
    correction_made: bool = False
    
    # Detalhes adicionais
    error_occurred: bool = False
    error_message: Optional[str] = None
    additional_metadata: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio para serializa√ß√£o"""
        return asdict(self)


class MetricsTracker:
    """Classe para rastrear m√©tricas durante execu√ß√µes"""
    
    def __init__(self, implementation_type: str):
        self.implementation_type = implementation_type
        self.current_metric: Optional[MetricData] = None
        self.start_time: Optional[float] = None
        
    def start_execution(self, user_input: str) -> str:
        """
        Inicia o rastreamento de uma nova execu√ß√£o
        
        Args:
            user_input: Input do usu√°rio
            
        Returns:
            execution_id: ID √∫nico da execu√ß√£o
        """
        execution_id = str(uuid.uuid4())
        timestamp = datetime.now().isoformat()
        
        self.current_metric = MetricData(
            execution_id=execution_id,
            timestamp=timestamp,
            implementation_type=self.implementation_type,
            user_input=user_input
        )
        
        self.start_time = time.time()
        
        print(f"üìä [MetricsTracker] Iniciando rastreamento - ID: {execution_id}")
        return execution_id
    
    def track_api_call(self, input_tokens: int, output_tokens: int):
        """
        Registra uma chamada de API
        
        Args:
            input_tokens: Tokens de entrada
            output_tokens: Tokens de sa√≠da
        """
        if not self.current_metric:
            print("‚ö†Ô∏è [MetricsTracker] Nenhuma execu√ß√£o ativa para rastrear API call")
            return
            
        self.current_metric.api_calls_count += 1
        self.current_metric.total_input_tokens += input_tokens
        self.current_metric.total_output_tokens += output_tokens
        self.current_metric.total_tokens += (input_tokens + output_tokens)
        
        print(f"üìû [MetricsTracker] API call #{self.current_metric.api_calls_count} - "
              f"Tokens: {input_tokens} in / {output_tokens} out")
    
    def track_function_call(self, function_name: str, params: Dict[str, Any], 
                           result: Any, validation_passed: bool):
        """
        Registra uma chamada de fun√ß√£o
        
        Args:
            function_name: Nome da fun√ß√£o chamada
            params: Par√¢metros da fun√ß√£o
            result: Resultado da fun√ß√£o
            validation_passed: Se a valida√ß√£o passou
        """
        if not self.current_metric:
            print("‚ö†Ô∏è [MetricsTracker] Nenhuma execu√ß√£o ativa para rastrear function call")
            return
            
        self.current_metric.function_called = function_name
        self.current_metric.function_params = params
        self.current_metric.function_result = result
        self.current_metric.validation_passed = validation_passed
        self.current_metric.has_function_call = True
        
        print(f"üîß [MetricsTracker] Fun√ß√£o chamada: {function_name} - "
              f"Valida√ß√£o: {'‚úÖ' if validation_passed else '‚ùå'}")
    
    def start_verification_phase(self):
        """Marca o in√≠cio da fase de verifica√ß√£o (Chain of Verification)"""
        if not self.current_metric:
            return
            
        self.current_metric.verification_used = True
        self.verification_start_time = time.time()
        print("üîç [MetricsTracker] Iniciando fase de verifica√ß√£o")
    
    def end_verification_phase(self, verification_tokens: int, correction_made: bool):
        """
        Marca o fim da fase de verifica√ß√£o
        
        Args:
            verification_tokens: Tokens usados na verifica√ß√£o
            correction_made: Se uma corre√ß√£o foi feita
        """
        if not self.current_metric or not hasattr(self, 'verification_start_time'):
            return
            
        verification_time = time.time() - self.verification_start_time
        self.current_metric.verification_latency_ms = verification_time * 1000
        self.current_metric.verification_tokens = verification_tokens
        self.current_metric.correction_made = correction_made
        
        print(f"‚úÖ [MetricsTracker] Verifica√ß√£o conclu√≠da - "
              f"Tempo: {verification_time*1000:.2f}ms - "
              f"Corre√ß√£o: {'‚úÖ' if correction_made else '‚ùå'}")
    
    def track_error(self, error_message: str):
        """
        Registra um erro durante a execu√ß√£o
        
        Args:
            error_message: Mensagem de erro
        """
        if not self.current_metric:
            return
            
        self.current_metric.error_occurred = True
        self.current_metric.error_message = error_message
        self.current_metric.response_complete = False
        
        print(f"‚ùå [MetricsTracker] Erro registrado: {error_message}")
    
    def end_execution(self, final_response: str, 
                     additional_metadata: Optional[Dict[str, Any]] = None) -> MetricData:
        """
        Finaliza o rastreamento e retorna os dados coletados
        
        Args:
            final_response: Resposta final gerada
            additional_metadata: Metadados adicionais
            
        Returns:
            MetricData: Dados coletados da execu√ß√£o
        """
        if not self.current_metric or not self.start_time:
            raise ValueError("Nenhuma execu√ß√£o ativa para finalizar")
            
        # Calcula lat√™ncia total
        total_time = time.time() - self.start_time
        self.current_metric.total_latency_ms = total_time * 1000
        
        # Armazena resposta final e metadados
        self.current_metric.final_response = final_response
        self.current_metric.additional_metadata = additional_metadata or {}
        
        # Verifica se a resposta est√° completa (heur√≠stica simples)
        if len(final_response.strip()) < 10:
            self.current_metric.response_complete = False
        
        print(f"üèÅ [MetricsTracker] Execu√ß√£o finalizada - "
              f"Tempo total: {total_time*1000:.2f}ms - "
              f"Tokens: {self.current_metric.total_tokens}")
        
        # Retorna uma c√≥pia dos dados
        result = MetricData(**asdict(self.current_metric))
        
        # Reset para pr√≥xima execu√ß√£o
        self.current_metric = None
        self.start_time = None
        
        return result
    
    def get_current_metrics(self) -> Optional[Dict[str, Any]]:
        """Retorna as m√©tricas atuais (para debug)"""
        if not self.current_metric:
            return None
        return self.current_metric.to_dict()


class MetricsAnalyzer:
    """Classe para analisar e comparar m√©tricas coletadas"""
    
    @staticmethod
    def compare_implementations(original_metrics: List[MetricData], 
                              cov_metrics: List[MetricData]) -> Dict[str, Any]:
        """
        Compara m√©tricas entre implementa√ß√µes
        
        Args:
            original_metrics: M√©tricas da implementa√ß√£o original
            cov_metrics: M√©tricas da implementa√ß√£o com Chain of Verification
            
        Returns:
            Relat√≥rio de compara√ß√£o
        """
        def calculate_avg(metrics: List[MetricData], field: str) -> float:
            values = [getattr(m, field) for m in metrics if hasattr(m, field)]
            return sum(values) / len(values) if values else 0.0
        
        def calculate_success_rate(metrics: List[MetricData]) -> float:
            if not metrics:
                return 0.0
            successful = sum(1 for m in metrics if not m.error_occurred and m.response_complete)
            return (successful / len(metrics)) * 100
        
        comparison = {
            "summary": {
                "original_executions": len(original_metrics),
                "cov_executions": len(cov_metrics),
                "timestamp": datetime.now().isoformat()
            },
            "latency": {
                "original_avg_ms": calculate_avg(original_metrics, "total_latency_ms"),
                "cov_avg_ms": calculate_avg(cov_metrics, "total_latency_ms"),
                "improvement_factor": None
            },
            "tokens": {
                "original_avg_total": calculate_avg(original_metrics, "total_tokens"),
                "cov_avg_total": calculate_avg(cov_metrics, "total_tokens"),
                "cov_avg_verification": calculate_avg(cov_metrics, "verification_tokens"),
                "efficiency_ratio": None
            },
            "quality": {
                "original_success_rate": calculate_success_rate(original_metrics),
                "cov_success_rate": calculate_success_rate(cov_metrics),
                "cov_correction_rate": None
            }
        }
        
        # Calcula fatores de melhoria
        if comparison["latency"]["original_avg_ms"] > 0:
            comparison["latency"]["improvement_factor"] = (
                comparison["latency"]["cov_avg_ms"] / comparison["latency"]["original_avg_ms"]
            )
        
        if comparison["tokens"]["original_avg_total"] > 0:
            comparison["tokens"]["efficiency_ratio"] = (
                comparison["tokens"]["cov_avg_total"] / comparison["tokens"]["original_avg_total"]
            )
        
        if cov_metrics:
            corrections_made = sum(1 for m in cov_metrics if m.correction_made)
            comparison["quality"]["cov_correction_rate"] = (corrections_made / len(cov_metrics)) * 100
        
        return comparison
    
    @staticmethod
    def generate_report(comparison: Dict[str, Any]) -> str:
        """Gera um relat√≥rio leg√≠vel da compara√ß√£o"""
        report = []
        report.append("üìä RELAT√ìRIO DE COMPARA√á√ÉO - ORIGINAL vs CHAIN OF VERIFICATION")
        report.append("=" * 70)
        
        # Resumo
        summary = comparison["summary"]
        report.append("\nüìà RESUMO:")
        report.append(f"   ‚Ä¢ Execu√ß√µes Original: {summary['original_executions']}")
        report.append(f"   ‚Ä¢ Execu√ß√µes CoV: {summary['cov_executions']}")
        report.append(f"   ‚Ä¢ Data: {summary['timestamp'][:19]}")
        
        # Lat√™ncia
        latency = comparison["latency"]
        report.append("\n‚è±Ô∏è LAT√äNCIA:")
        report.append(f"   ‚Ä¢ Original: {latency['original_avg_ms']:.2f}ms")
        report.append(f"   ‚Ä¢ CoV: {latency['cov_avg_ms']:.2f}ms")
        if latency["improvement_factor"]:
            factor = latency["improvement_factor"]
            change = "mais lenta" if factor > 1 else "mais r√°pida"
            report.append(f"   ‚Ä¢ CoV √© {abs(factor-1)*100:.1f}% {change}")
        
        # Tokens
        tokens = comparison["tokens"]
        report.append("\nüî§ TOKENS:")
        report.append(f"   ‚Ä¢ Original (m√©dia): {tokens['original_avg_total']:.0f}")
        report.append(f"   ‚Ä¢ CoV (m√©dia): {tokens['cov_avg_total']:.0f}")
        report.append(f"   ‚Ä¢ CoV verifica√ß√£o: {tokens['cov_avg_verification']:.0f}")
        if tokens["efficiency_ratio"]:
            ratio = tokens["efficiency_ratio"]
            report.append(f"   ‚Ä¢ CoV usa {ratio:.2f}x mais tokens")
        
        # Qualidade
        quality = comparison["quality"]
        report.append("\n‚úÖ QUALIDADE:")
        report.append(f"   ‚Ä¢ Taxa sucesso Original: {quality['original_success_rate']:.1f}%")
        report.append(f"   ‚Ä¢ Taxa sucesso CoV: {quality['cov_success_rate']:.1f}%")
        if quality["cov_correction_rate"] is not None:
            report.append(f"   ‚Ä¢ CoV fez corre√ß√µes: {quality['cov_correction_rate']:.1f}%")
        
        return "\n".join(report)
