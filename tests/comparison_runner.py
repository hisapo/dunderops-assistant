"""
Sistema de comparaÃ§Ã£o entre implementaÃ§Ã£o original e Chain of Verification
"""

import json
import os
import sys
import time
from typing import List, Dict, Any
from openai import OpenAI

# Adiciona o diretÃ³rio pai ao path para imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Imports das implementaÃ§Ãµes
from src.core.prompt_config import PromptConfig
from src.core.function_validator import FunctionValidator
from src.core.metrics_tracker import MetricsTracker
from src.core.experiment_logger import ExperimentLogger, ExperimentSession
from faithful_implementations import (
    FormUIOriginalReproduction, 
    FormUICoVReproduction, 
    FormUISecureReproduction
)

# Carrega variÃ¡veis de ambiente do .env
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # Se dotenv nÃ£o estiver disponÃ­vel, continua sem ele
    pass


class ComparisonRunner:
    """Executa comparaÃ§Ãµes entre as duas implementaÃ§Ãµes"""
    
    def __init__(self):
        self.client = self._setup_client()
        self.prompts = PromptConfig()
        self.validator = FunctionValidator(self.prompts)
        self.logger = ExperimentLogger()
        
        # Inicializa implementaÃ§Ãµes FIÃ‰IS aos form_ui
        self.original = FormUIOriginalReproduction(self.client, self.prompts, self.validator)
        self.cov = FormUICoVReproduction(self.client, self.prompts, self.validator)
        self.secure = FormUISecureReproduction(self.client, self.prompts, self.validator)
        
        # Carrega manifest
        with open("config/manifest.json") as f:
            self.manifest = json.load(f)
    
    def _setup_client(self) -> OpenAI:
        """Configura cliente OpenAI"""
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY nÃ£o configurada")
        return OpenAI(api_key=api_key)
    
    def run_single_comparison(self, user_input: str) -> Dict[str, Any]:
        """Executa uma comparaÃ§Ã£o para um input especÃ­fico"""
        
        print(f"\n{'='*60}")
        print(f"ğŸ†š COMPARANDO: {user_input}")
        print(f"{'='*60}")
        
        results = {}
        
        # Executa versÃ£o original
        print("\nğŸ”µ EXECUTANDO VERSÃƒO ORIGINAL...")
        original_tracker = MetricsTracker("original")
        original_tracker.start_execution(user_input)
        
        try:
            original_response = self.original.process_request(user_input, self.manifest, original_tracker)
            original_metric = original_tracker.end_execution(original_response)
            results["original"] = {
                "response": original_response,
                "metric": original_metric,
                "success": True
            }
            print(f"âœ… Original concluÃ­da: {original_metric.total_latency_ms:.2f}ms, {original_metric.total_tokens} tokens")
        except Exception as e:
            original_tracker.track_error(str(e))
            original_metric = original_tracker.end_execution(f"Erro: {str(e)}")
            results["original"] = {
                "response": f"Erro: {str(e)}",
                "metric": original_metric,
                "success": False
            }
            print(f"âŒ Original falhou: {str(e)}")
        
        # Executa versÃ£o CoV
        print("\nğŸ” EXECUTANDO VERSÃƒO CHAIN OF VERIFICATION...")
        cov_tracker = MetricsTracker("chain_of_verification")
        cov_tracker.start_execution(user_input)
        
        try:
            cov_response = self.cov.process_request(user_input, self.manifest, cov_tracker)
            cov_metric = cov_tracker.end_execution(cov_response)
            results["cov"] = {
                "response": cov_response,
                "metric": cov_metric,
                "success": True
            }
            print(f"âœ… CoV concluÃ­da: {cov_metric.total_latency_ms:.2f}ms, {cov_metric.total_tokens} tokens")
            print(f"   VerificaÃ§Ã£o: {cov_metric.verification_used}, CorreÃ§Ã£o: {cov_metric.correction_made}")
        except Exception as e:
            cov_tracker.track_error(str(e))
            cov_metric = cov_tracker.end_execution(f"Erro: {str(e)}")
            results["cov"] = {
                "response": f"Erro: {str(e)}",
                "metric": cov_metric,
                "success": False
            }
            print(f"âŒ CoV falhou: {str(e)}")
        
        # Compara resultados
        print("\nğŸ“Š COMPARAÃ‡ÃƒO:")
        if results["original"]["success"] and results["cov"]["success"]:
            orig_metric = results["original"]["metric"]
            cov_metric = results["cov"]["metric"]
            
            latency_diff = ((cov_metric.total_latency_ms - orig_metric.total_latency_ms) / orig_metric.total_latency_ms) * 100
            token_diff = ((cov_metric.total_tokens - orig_metric.total_tokens) / orig_metric.total_tokens) * 100
            
            print(f"   â±ï¸ LatÃªncia: CoV {latency_diff:+.1f}% vs Original")
            print(f"   ğŸ”¤ Tokens: CoV {token_diff:+.1f}% vs Original")
            print(f"   ğŸ” VerificaÃ§Ã£o CoV: {cov_metric.verification_used}")
            print(f"   ğŸ”§ CorreÃ§Ã£o CoV: {cov_metric.correction_made}")
        
        print("\nğŸ“ RESPOSTAS:")
        print(f"   ğŸ”µ Original: {results['original']['response'][:100]}...")
        print(f"   ğŸ” CoV: {results['cov']['response'][:100]}...")
        
        return results
    
    def run_experiment(self, test_cases: List[str], experiment_name: str) -> str:
        """Executa experimento completo com mÃºltiplos casos de teste"""
        
        print(f"ğŸ¯ INICIANDO EXPERIMENTO: {experiment_name}")
        print(f"ğŸ“‹ Casos de teste: {len(test_cases)}")
        
        session = ExperimentSession(experiment_name, self.logger)
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n{'='*20} CASO {i}/{len(test_cases)} {'='*20}")
            
            try:
                results = self.run_single_comparison(test_case)
                
                # Adiciona mÃ©tricas Ã  sessÃ£o
                if results["original"]["success"]:
                    session.add_original_metric(results["original"]["metric"])
                
                if results["cov"]["success"]:
                    session.add_cov_metric(results["cov"]["metric"])
                
                # Pausa entre testes para evitar rate limiting
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ Erro no caso de teste {i}: {str(e)}")
        
        # Finaliza experimento
        comparison_file = session.finalize_experiment()
        
        print("\nğŸ EXPERIMENTO CONCLUÃDO!")
        print(f"ğŸ“Š Arquivo de comparaÃ§Ã£o: {comparison_file}")
        
        return comparison_file


def get_default_test_cases() -> List[str]:
    """Retorna casos de teste padrÃ£o para comparaÃ§Ã£o"""
    return [
        # Casos com parÃ¢metros completos
        "Agendar reuniÃ£o sobre vendas para 2024-01-15 Ã s 14:00 na Conference Room",
        "Gerar orÃ§amento para 1000 folhas de papel A4 120gsm",
        "Sugerir pegadinha no Dwight tipo desk com orÃ§amento de $20",
        
        # Casos com parÃ¢metros incompletos
        "Preciso agendar uma reuniÃ£o sobre vendas",
        "Quanto custa papel A4?",
        "Quero fazer uma pegadinha no Dwight",
        
        # Casos sem funÃ§Ã£o
        "Quem Ã© o Michael Scott?",
        "Como estÃ¡ o time de vendas hoje?",
        "Qual Ã© a polÃ­tica de feriados da empresa?",
        
        # Casos complexos
        "Agendar reuniÃ£o urgente sobre o budget Q4 amanhÃ£ de manhÃ£",
        "Preciso de papel para impressÃ£o em massa, barato mas qualidade boa"
    ]


if __name__ == "__main__":
    try:
        print("ğŸš€ SISTEMA DE COMPARAÃ‡ÃƒO DUNDEROPS")
        print("=" * 50)
        
        runner = ComparisonRunner()
        test_cases = get_default_test_cases()
        
        # Executa experimento padrÃ£o
        experiment_file = runner.run_experiment(test_cases, "baseline_comparison")
        
        print("\nğŸ‰ ComparaÃ§Ã£o concluÃ­da!")
        print(f"ğŸ“„ RelatÃ³rio salvo em: {experiment_file}")
        
    except ValueError as e:
        print(f"âŒ Erro de configuraÃ§Ã£o: {str(e)}")
        print("ğŸ’¡ Configure a variÃ¡vel OPENAI_API_KEY para executar comparaÃ§Ãµes")
    except Exception as e:
        print(f"âŒ Erro inesperado: {str(e)}")
